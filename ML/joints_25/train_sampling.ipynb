{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras.layers import LSTM, CuDNNLSTM\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "import keras.metrics\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 25 # timestep\n",
    "# choose_joints = np.array([ 8, 6, 5, 12, 10, 9, 15, 14, 13, 19, 18, 17, 4, 3 ]) - 1\n",
    "choose_joints = np.array([ 8, 6, 5, 12, 10, 9]) - 1\n",
    "select_column = []\n",
    "for i in range(len(choose_joints)): # 14 body join( except waist)\n",
    "    select_column.append(0 + 3*choose_joints[i]) # select x\n",
    "    select_column.append(1 + 3*choose_joints[i]) # select y\n",
    "    select_column.append(2 + 3*choose_joints[i]) # select z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = \"F:/Master Project/Dataset/Extract_Data/25 joints\"\n",
    "f_x = open(path_save+\"/train_x.pickle\",'rb')\n",
    "f_y = open(path_save+\"/train_y.pickle\",'rb')\n",
    "origin_train_x = pickle.load(f_x)\n",
    "origin_train_y = np.array(pickle.load(f_y))\n",
    "\n",
    "f_x = open(path_save+\"/test_x.pickle\",'rb')\n",
    "f_y = open(path_save+\"/test_y.pickle\",'rb')\n",
    "origin_test_x = pickle.load(f_x)\n",
    "origin_test_y = np.array(pickle.load(f_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66330, 18)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(origin_train_x[0].shape)\n",
    "print(len(origin_test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reform25to14(data):\n",
    "    new_data = []\n",
    "    # data shape (file,frame,75)\n",
    "    for file in data:\n",
    "        new_file = file[:,select_column]\n",
    "#         import pdb;pdb.set_trace()\n",
    "        new_data.append(new_file)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_train_x = reform25to14(origin_train_x)\n",
    "origin_test_x = reform25to14(origin_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin_test_x[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_x(x):\n",
    "    \n",
    "    frames = x\n",
    "    \n",
    "    random_sample_range = 3 # sampling value is not more than 3\n",
    "\n",
    "    # Randomly choose sample interval and start frame\n",
    "    sample_interval = np.random.randint(1, random_sample_range + 1)\n",
    "\n",
    "    start_i = np.random.randint(0, len(frames) - sample_interval * sequence_length + 1)\n",
    "\n",
    "    # Extract frames as tensors\n",
    "    image_sequence = []\n",
    "    end_i = sample_interval * sequence_length + start_i\n",
    "    for i in range(start_i, end_i, sample_interval):\n",
    "        # image_path = frames[i]\n",
    "        if len(image_sequence) < sequence_length:\n",
    "            image_sequence.append(frames[i])\n",
    "        else:\n",
    "            break\n",
    "    image_sequence = np.array(image_sequence)   \n",
    "    return image_sequence\n",
    "\n",
    "# Use for sampling and reforming data for sending to ML model\n",
    "def reform_to_sequence(data_x, data_y, is_training):\n",
    "    \n",
    "    if is_training:\n",
    "        random_time = 20000\n",
    "        output_x = np.zeros((len(data_x)*random_time, sequence_length, data_x[0].shape[-1]) ) #(len,timestep, 28)\n",
    "        \n",
    "    else:        \n",
    "        random_time = 10000\n",
    "        output_x = np.zeros((len(data_x)*random_time, sequence_length, data_x[0].shape[-1]) ) #(len*random_time,timestep, 28)\n",
    "    \n",
    "    count = 0\n",
    "    output_y = np.arange( len(data_y)*random_time ) # create array\n",
    "    \n",
    "    # sampling window-data in random_time time\n",
    "    for n_time in range(random_time):\n",
    "        for i,x in enumerate(data_x):\n",
    "            sequence = sampling_x(x)\n",
    "            output_x[count] = sequence\n",
    "            output_y[count] = data_y[i]\n",
    "            count += 1\n",
    "    \n",
    "    # output_x - x_data   : shape(num_of_file * random_time, sequence_length, 75)\n",
    "    # output_y - y_data   : shape(num_of_file * random_time, sequence_length, 75)\n",
    "    return output_x, output_y     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, number_feature):\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(50, input_shape=(sequence_length, number_feature),return_sequences=False))\n",
    "    model.add(Dropout(0.4))#使用Dropout函数可以使模型有更多的机会学习到多种独立的表征\n",
    "    model.add(Dense(60) )\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_feature = origin_train_x[0].shape[-1] # last index\n",
    "\n",
    "load_model = False\n",
    "model = create_model(sequence_length, number_feature)\n",
    "start_epoch = 0\n",
    "\n",
    "if load_model:\n",
    "    weights_path = 'weight-sampling-01-0.95.hdf5'    \n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "filepath=\"weight-sampling-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True,)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y  = reform_to_sequence(origin_test_x, origin_test_y, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, train_y = reform_to_sequence(origin_train_x, origin_train_y, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139808/140000 [============================>.] - ETA: 0s - loss: 0.7384 - accuracy: 0.7201\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.76870, saving model to weight-sampling-01-0.77.hdf5\n",
      "140000/140000 [==============================] - 35s 248us/sample - loss: 0.7379 - accuracy: 0.7203 - val_loss: 0.5850 - val_accuracy: 0.7687\n",
      "-----------------------\n",
      "epoch:  2\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139872/140000 [============================>.] - ETA: 0s - loss: 0.4556 - accuracy: 0.8400\n",
      "Epoch 00001: val_accuracy improved from 0.76870 to 0.89413, saving model to weight-sampling-01-0.89.hdf5\n",
      "140000/140000 [==============================] - 36s 259us/sample - loss: 0.4556 - accuracy: 0.8400 - val_loss: 0.3135 - val_accuracy: 0.8941\n",
      "-----------------------\n",
      "epoch:  3\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139904/140000 [============================>.] - ETA: 0s - loss: 0.4202 - accuracy: 0.8539\n",
      "Epoch 00001: val_accuracy improved from 0.89413 to 0.89435, saving model to weight-sampling-01-0.89.hdf5\n",
      "140000/140000 [==============================] - 38s 275us/sample - loss: 0.4201 - accuracy: 0.8539 - val_loss: 0.2722 - val_accuracy: 0.8943\n",
      "-----------------------\n",
      "epoch:  4\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139904/140000 [============================>.] - ETA: 0s - loss: 0.2685 - accuracy: 0.9082\n",
      "Epoch 00001: val_accuracy did not improve from 0.89435\n",
      "140000/140000 [==============================] - 40s 283us/sample - loss: 0.2685 - accuracy: 0.9082 - val_loss: 0.3814 - val_accuracy: 0.8637\n",
      "-----------------------\n",
      "epoch:  5\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139840/140000 [============================>.] - ETA: 0s - loss: 0.2365 - accuracy: 0.9182\n",
      "Epoch 00001: val_accuracy improved from 0.89435 to 0.91383, saving model to weight-sampling-01-0.91.hdf5\n",
      "140000/140000 [==============================] - 40s 288us/sample - loss: 0.2364 - accuracy: 0.9182 - val_loss: 0.2306 - val_accuracy: 0.9138\n",
      "-----------------------\n",
      "epoch:  6\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139808/140000 [============================>.] - ETA: 0s - loss: 0.1759 - accuracy: 0.9397\n",
      "Epoch 00001: val_accuracy improved from 0.91383 to 0.91718, saving model to weight-sampling-01-0.92.hdf5\n",
      "140000/140000 [==============================] - 41s 294us/sample - loss: 0.1761 - accuracy: 0.9397 - val_loss: 0.2526 - val_accuracy: 0.9172\n",
      "-----------------------\n",
      "epoch:  7\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139936/140000 [============================>.] - ETA: 0s - loss: 0.1967 - accuracy: 0.9337\n",
      "Epoch 00001: val_accuracy improved from 0.91718 to 0.92710, saving model to weight-sampling-01-0.93.hdf5\n",
      "140000/140000 [==============================] - 42s 302us/sample - loss: 0.1968 - accuracy: 0.9337 - val_loss: 0.1955 - val_accuracy: 0.9271\n",
      "-----------------------\n",
      "epoch:  8\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139680/140000 [============================>.] - ETA: 0s - loss: 0.2663 - accuracy: 0.9107\n",
      "Epoch 00001: val_accuracy did not improve from 0.92710\n",
      "140000/140000 [==============================] - 48s 344us/sample - loss: 0.2665 - accuracy: 0.9106 - val_loss: 0.3287 - val_accuracy: 0.8587\n",
      "-----------------------\n",
      "epoch:  9\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139968/140000 [============================>.] - ETA: 0s - loss: 0.2532 - accuracy: 0.9140\n",
      "Epoch 00001: val_accuracy did not improve from 0.92710\n",
      "140000/140000 [==============================] - 42s 302us/sample - loss: 0.2532 - accuracy: 0.9141 - val_loss: 0.3640 - val_accuracy: 0.8757\n",
      "-----------------------\n",
      "epoch:  10\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139968/140000 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9401\n",
      "Epoch 00001: val_accuracy did not improve from 0.92710\n",
      "140000/140000 [==============================] - 43s 305us/sample - loss: 0.1797 - accuracy: 0.9401 - val_loss: 0.5212 - val_accuracy: 0.8533\n",
      "-----------------------\n",
      "epoch:  11\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139744/140000 [============================>.] - ETA: 0s - loss: 0.1475 - accuracy: 0.9501\n",
      "Epoch 00001: val_accuracy did not improve from 0.92710\n",
      "140000/140000 [==============================] - 43s 307us/sample - loss: 0.1474 - accuracy: 0.9501 - val_loss: 0.3074 - val_accuracy: 0.9057\n",
      "-----------------------\n",
      "epoch:  12\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139840/140000 [============================>.] - ETA: 0s - loss: 0.1331 - accuracy: 0.9562\n",
      "Epoch 00001: val_accuracy improved from 0.92710 to 0.93718, saving model to weight-sampling-01-0.94.hdf5\n",
      "140000/140000 [==============================] - 43s 309us/sample - loss: 0.1331 - accuracy: 0.9562 - val_loss: 0.2152 - val_accuracy: 0.9372\n",
      "-----------------------\n",
      "epoch:  13\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139904/140000 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9594\n",
      "Epoch 00001: val_accuracy did not improve from 0.93718\n",
      "140000/140000 [==============================] - 43s 309us/sample - loss: 0.1222 - accuracy: 0.9594 - val_loss: 0.3248 - val_accuracy: 0.9050\n",
      "-----------------------\n",
      "epoch:  14\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139808/140000 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.9578\n",
      "Epoch 00001: val_accuracy did not improve from 0.93718\n",
      "140000/140000 [==============================] - 43s 307us/sample - loss: 0.1315 - accuracy: 0.9578 - val_loss: 0.3044 - val_accuracy: 0.9054\n",
      "-----------------------\n",
      "epoch:  15\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139936/140000 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9641\n",
      "Epoch 00001: val_accuracy did not improve from 0.93718\n",
      "140000/140000 [==============================] - 45s 324us/sample - loss: 0.1121 - accuracy: 0.9641 - val_loss: 0.2365 - val_accuracy: 0.9214\n",
      "-----------------------\n",
      "epoch:  16\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139936/140000 [============================>.] - ETA: 0s - loss: 0.1043 - accuracy: 0.9660\n",
      "Epoch 00001: val_accuracy did not improve from 0.93718\n",
      "140000/140000 [==============================] - 45s 323us/sample - loss: 0.1043 - accuracy: 0.9660 - val_loss: 0.3019 - val_accuracy: 0.9216\n",
      "-----------------------\n",
      "epoch:  17\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139968/140000 [============================>.] - ETA: 0s - loss: 0.0972 - accuracy: 0.9685\n",
      "Epoch 00001: val_accuracy did not improve from 0.93718\n",
      "140000/140000 [==============================] - 48s 342us/sample - loss: 0.0972 - accuracy: 0.9686 - val_loss: 0.4967 - val_accuracy: 0.8881\n",
      "-----------------------\n",
      "epoch:  18\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139840/140000 [============================>.] - ETA: 0s - loss: 0.0957 - accuracy: 0.9697\n",
      "Epoch 00001: val_accuracy did not improve from 0.93718\n",
      "140000/140000 [==============================] - 46s 330us/sample - loss: 0.0958 - accuracy: 0.9697 - val_loss: 0.3856 - val_accuracy: 0.8964\n",
      "-----------------------\n",
      "epoch:  19\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139936/140000 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9708\n",
      "Epoch 00001: val_accuracy improved from 0.93718 to 0.94885, saving model to weight-sampling-01-0.95.hdf5\n",
      "140000/140000 [==============================] - 48s 345us/sample - loss: 0.0897 - accuracy: 0.9708 - val_loss: 0.1721 - val_accuracy: 0.9488\n",
      "-----------------------\n",
      "epoch:  20\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139840/140000 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9720\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 48s 343us/sample - loss: 0.0870 - accuracy: 0.9720 - val_loss: 0.2554 - val_accuracy: 0.9282\n",
      "-----------------------\n",
      "epoch:  21\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139936/140000 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9747\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 48s 343us/sample - loss: 0.0783 - accuracy: 0.9747 - val_loss: 0.2681 - val_accuracy: 0.9178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "epoch:  22\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139936/140000 [============================>.] - ETA: 0s - loss: 0.1001 - accuracy: 0.9685\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 47s 338us/sample - loss: 0.1001 - accuracy: 0.9685 - val_loss: 0.2666 - val_accuracy: 0.9123\n",
      "-----------------------\n",
      "epoch:  23\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139968/140000 [============================>.] - ETA: 0s - loss: 0.0806 - accuracy: 0.9750\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 48s 341us/sample - loss: 0.0807 - accuracy: 0.9750 - val_loss: 0.3991 - val_accuracy: 0.9050\n",
      "-----------------------\n",
      "epoch:  24\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139872/140000 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9767\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 47s 333us/sample - loss: 0.0745 - accuracy: 0.9767 - val_loss: 0.3164 - val_accuracy: 0.9174\n",
      "-----------------------\n",
      "epoch:  25\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139840/140000 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9776\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 46s 331us/sample - loss: 0.0740 - accuracy: 0.9776 - val_loss: 0.4564 - val_accuracy: 0.8999\n",
      "-----------------------\n",
      "epoch:  26\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139936/140000 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9783\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 47s 333us/sample - loss: 0.0687 - accuracy: 0.9784 - val_loss: 0.3853 - val_accuracy: 0.9109\n",
      "-----------------------\n",
      "epoch:  27\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139872/140000 [============================>.] - ETA: 0s - loss: 0.0698 - accuracy: 0.9782\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 50s 354us/sample - loss: 0.0698 - accuracy: 0.9783 - val_loss: 0.1860 - val_accuracy: 0.9412\n",
      "-----------------------\n",
      "epoch:  28\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139808/140000 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9800\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 47s 336us/sample - loss: 0.0622 - accuracy: 0.9800 - val_loss: 0.3312 - val_accuracy: 0.9098\n",
      "-----------------------\n",
      "epoch:  29\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139872/140000 [============================>.] - ETA: 0s - loss: 0.0633 - accuracy: 0.9801\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 45s 324us/sample - loss: 0.0633 - accuracy: 0.9801 - val_loss: 0.2879 - val_accuracy: 0.9168\n",
      "-----------------------\n",
      "epoch:  30\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139840/140000 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9807\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 43s 306us/sample - loss: 0.0623 - accuracy: 0.9807 - val_loss: 0.3249 - val_accuracy: 0.9201\n",
      "-----------------------\n",
      "epoch:  31\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139872/140000 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9810\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 41s 296us/sample - loss: 0.0608 - accuracy: 0.9810 - val_loss: 0.3184 - val_accuracy: 0.9150\n",
      "-----------------------\n",
      "epoch:  32\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139840/140000 [============================>.] - ETA: 0s - loss: 0.0599 - accuracy: 0.9818\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 42s 301us/sample - loss: 0.0599 - accuracy: 0.9818 - val_loss: 0.3060 - val_accuracy: 0.9161\n",
      "-----------------------\n",
      "epoch:  33\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139904/140000 [============================>.] - ETA: 0s - loss: 0.0586 - accuracy: 0.9818\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 42s 300us/sample - loss: 0.0586 - accuracy: 0.9818 - val_loss: 0.2706 - val_accuracy: 0.9302\n",
      "-----------------------\n",
      "epoch:  34\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139904/140000 [============================>.] - ETA: 0s - loss: 0.0539 - accuracy: 0.9833\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 44s 313us/sample - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.2936 - val_accuracy: 0.9270\n",
      "-----------------------\n",
      "epoch:  35\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139968/140000 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9839\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 44s 314us/sample - loss: 0.0531 - accuracy: 0.9839 - val_loss: 0.4267 - val_accuracy: 0.9084\n",
      "-----------------------\n",
      "epoch:  36\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139808/140000 [============================>.] - ETA: 0s - loss: 0.0512 - accuracy: 0.9843\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 46s 326us/sample - loss: 0.0512 - accuracy: 0.9844 - val_loss: 0.3366 - val_accuracy: 0.9232\n",
      "-----------------------\n",
      "epoch:  37\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139776/140000 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9830\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 44s 316us/sample - loss: 0.0562 - accuracy: 0.9830 - val_loss: 0.2695 - val_accuracy: 0.9335\n",
      "-----------------------\n",
      "epoch:  38\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139936/140000 [============================>.] - ETA: 0s - loss: 0.0540 - accuracy: 0.9836\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 44s 311us/sample - loss: 0.0540 - accuracy: 0.9836 - val_loss: 0.4339 - val_accuracy: 0.9028\n",
      "-----------------------\n",
      "epoch:  39\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139904/140000 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9816\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 44s 315us/sample - loss: 0.0605 - accuracy: 0.9816 - val_loss: 0.2072 - val_accuracy: 0.9380\n",
      "-----------------------\n",
      "epoch:  40\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139936/140000 [============================>.] - ETA: 0s - loss: 0.0512 - accuracy: 0.9842\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 46s 330us/sample - loss: 0.0512 - accuracy: 0.9842 - val_loss: 0.3367 - val_accuracy: 0.9207\n",
      "-----------------------\n",
      "epoch:  41\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      "139936/140000 [============================>.] - ETA: 0s - loss: 0.0487 - accuracy: 0.9854\n",
      "Epoch 00001: val_accuracy did not improve from 0.94885\n",
      "140000/140000 [==============================] - 45s 320us/sample - loss: 0.0487 - accuracy: 0.9854 - val_loss: 0.4849 - val_accuracy: 0.8925\n",
      "-----------------------\n",
      "epoch:  42\n",
      "Train on 140000 samples, validate on 60000 samples\n",
      " 23776/140000 [====>.........................] - ETA: 27s - loss: 0.0466 - accuracy: 0.986"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    173\u001b[0m       \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m       \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\peachman\\appdata\\local\\programs\\python\\python37\\Lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[1;34m(self, step, mode, size)\u001b[0m\n\u001b[0;32m    700\u001b[0m             mode, 'end', step, batch_logs)\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    759\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    388\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynamic_display\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\b'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mprev_total_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-981f01e585bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreform_to_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin_train_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin_train_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     model.fit(train_x, train_y, epochs=start_epoch+1,\n\u001b[1;32m----> 8\u001b[1;33m              validation_data=(test_x,test_y), callbacks=callbacks_list, initial_epoch=start_epoch)\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-----------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m                       total_epochs=1)\n\u001b[0;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 372\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\peachman\\appdata\\local\\programs\\python\\python37\\Lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    683\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    963\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_worker_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    982\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m    983\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 984\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\master project\\environment\\action_reg\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     if not multi_worker_util.in_multi_worker_mode(\n\u001b[0;32m   1019\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1020\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1021\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_accuracy'"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "for i_ep in range(start_epoch+1,num_epoch):\n",
    "    \n",
    "    print('epoch: ', i_ep)\n",
    "    train_x, train_y = reform_to_sequence(origin_train_x, origin_train_y, is_training=True)\n",
    "    model.fit(train_x, train_y, epochs=start_epoch+1,\n",
    "             validation_data=(test_x,test_y), callbacks=callbacks_list, initial_epoch=start_epoch)\n",
    "    print(\"-----------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
