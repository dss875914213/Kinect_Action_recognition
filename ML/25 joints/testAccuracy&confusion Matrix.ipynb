{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'data_helper' from '__main__' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8f72ed14051a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'data_helper' from '__main__' (unknown location)"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import CuDNNLSTM\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 25 # timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = \"F:/Master Project/Dataset/Extract_Data/25 joints\"\n",
    "f_x = open(path_save+\"/test_x.pickle\",'rb')\n",
    "f_y = open(path_save+\"/test_y.pickle\",'rb')\n",
    "origin_test_x = pickle.load(f_x)\n",
    "origin_test_y = np.array(pickle.load(f_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_x(x):\n",
    "    \n",
    "    frames = x\n",
    "    \n",
    "    random_sample_range = 3\n",
    "\n",
    "    # Randomly choose sample interval and start frame\n",
    "    sample_interval = np.random.randint(1, random_sample_range + 1)\n",
    "\n",
    "    start_i = np.random.randint(0, len(frames) - sample_interval * sequence_length + 1)\n",
    "\n",
    "    # Extract frames as tensors\n",
    "    image_sequence = []\n",
    "#     for i in range(start_i, len(frames), sample_interval):\n",
    "    end_i = sample_interval * sequence_length + start_i\n",
    "    for i in range(start_i, end_i, sample_interval):\n",
    "        # image_path = frames[i]\n",
    "        if len(image_sequence) < sequence_length:\n",
    "            image_sequence.append(frames[i])\n",
    "        else:\n",
    "            break\n",
    "    image_sequence = np.array(image_sequence)   \n",
    "    return image_sequence\n",
    "\n",
    "\n",
    "def reform_to_sequence(data_x, data_y, is_training):\n",
    "    \n",
    "    if is_training:\n",
    "        random_time = 20000\n",
    "        output_x = np.zeros((len(data_x)*random_time, sequence_length, data_x[0].shape[-1]) ) #(len,timestep, 28)\n",
    "        \n",
    "    else:        \n",
    "        random_time = 10000\n",
    "        output_x = np.zeros((len(data_x)*random_time, sequence_length, data_x[0].shape[-1]) ) #(len*random_time,timestep, 28)\n",
    "    \n",
    "    count = 0\n",
    "    output_y = np.arange( len(data_y)*random_time ) # create array\n",
    "    for n_time in range(random_time):\n",
    "        for i,x in enumerate(data_x):\n",
    "            sequence = sampling_x(x)\n",
    "            output_x[count] = sequence\n",
    "            output_y[count] = data_y[i]\n",
    "            count += 1\n",
    "#     import pdb; pdb.set_trace()\n",
    "    return output_x, output_y     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([origin_test_x[4] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'data_helper' from '__main__' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-808a502f4510>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0morigin_test_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreform25to14\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin_test_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'6'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'data_helper' from '__main__' (unknown location)"
     ]
    }
   ],
   "source": [
    "from . import data_helper \n",
    "origin_test_x = reform25to14(origin_test_x,'6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = np.array([0,1,2,3,5])\n",
    "# origin_test_x2 = origin_test_x[index]\n",
    "# origin_test_y2 = origin_test_y[index]\n",
    "index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x, test_y  = reform_to_sequence( [origin_test_x[index] ] , [origin_test_y[index]], is_training=False)\n",
    "test_x, test_y  = reform_to_sequence( origin_test_x  , origin_test_y, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_frame, num_joint, num_output):\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(50, input_shape=(num_frame, num_joint),return_sequences=False))\n",
    "    model.add(Dropout(0.4))#使用Dropout函数可以使模型有更多的机会学习到多种独立的表征\n",
    "    model.add(Dense(60) )\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_output, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = 'weight-sampling-01-0.96.hdf5' # 15 frame\n",
    "max_frame = 25\n",
    "num_joint = 6\n",
    "model = create_model(max_frame, num_joint*3, 4)\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_x, y = test_y, batch_size=384, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = True\n",
    "\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "if normalize:\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"Normalized confusion matrix\")\n",
    "else:\n",
    "    print('Confusion matrix, without normalization')\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = ['拍球','投球','传球','站立']\n",
    "classes = ['dribble','shoot','pass','stand']\n",
    "title = 'test'\n",
    "cmap=plt.cm.Blues\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "# We want to show all ticks...\n",
    "# ax.figure.set_size_inches(10, 10)\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       # ... and label them with the respective list entries\n",
    "       xticklabels=classes, yticklabels=classes,\n",
    "       title=title,\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label')\n",
    "\n",
    "\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "fmt = '.2f' if normalize else 'd'\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], fmt),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sn\n",
    "# array = [[33,2,0,0,0,0,0,0,0,1,3], \n",
    "#         [3,31,0,0,0,0,0,0,0,0,0], \n",
    "#         [0,4,41,0,0,0,0,0,0,0,1], \n",
    "#         [0,1,0,30,0,6,0,0,0,0,1], \n",
    "#         [0,0,0,0,38,10,0,0,0,0,0], \n",
    "#         [0,0,0,3,1,39,0,0,0,0,4], \n",
    "#         [0,2,2,0,4,1,31,0,0,0,2],\n",
    "#         [0,1,0,0,0,0,0,36,0,2,0], \n",
    "#         [0,0,0,0,0,0,1,5,37,5,1], \n",
    "#         [3,0,0,0,0,0,0,0,0,39,0], \n",
    "#         [0,0,0,0,0,0,0,0,0,0,38]]\n",
    "# df_cm = pd.DataFrame(array, index = [i for i in \"ABCDEFGHIJK\"],\n",
    "#                   columns = [i for i in \"ABCDEFGHIJK\"])\n",
    "# plt.figure(figsize = (10,7))\n",
    "# sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "e897cbbc-7719-4511-b5ed-4f4549213667"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
